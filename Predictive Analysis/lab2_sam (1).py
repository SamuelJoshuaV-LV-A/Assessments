# -*- coding: utf-8 -*-
"""Lab2_Sam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iaZJ1_9CJGh74WdUpRV7JqLq-T6lDq1H
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from math import sqrt

#Loading data

df = pd.read_csv("/content/mushroom.csv")
df

#Checking for null values

df.isnull().sum()

#Using imputation to fill the missing value detected

from sklearn.impute import KNNImputer
num_cols = df.select_dtypes(include=['int64','float64']).columns

imputing = KNNImputer()
for i in num_cols:
  df[i] = imputing.fit_transform(df[[i]])

df.isnull().sum()

df.duplicated().sum()

# if duplicates are present
df = df.drop_duplicates()

df.shape

#comprehensive overview of the data

df.describe(include='all')

#Box plot to check if outliers are present

num_cols = df.select_dtypes(include=['int64','float64']).columns
for i in num_cols:
  sns.boxplot(df[i])
  plt.show()

#Removing outliers

Q1 = df[num_cols].quantile(0.25)
Q3 = df[num_cols].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outlier = ((df[num_cols]<lower_bound )| (df[num_cols]>upper_bound)).any(axis=1)
df = df[~outlier]

# after outlier removal
for i in num_cols:
  sns.boxplot(df[i])
  plt.show()

#EDA (Univaraite analysis)
for i in num_cols:
  sns.histplot(df[i])
  plt.show()

#bivaraite analysis using scatter plot for EDA and understanding the data in depth

for i in range(len(num_cols)):
    for j in range(i + 1, len(num_cols)):
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=df, x=num_cols[i], y=num_cols[j])
        plt.title(f'Scatter Plot between {num_cols[i]} and {num_cols[j]}')
        plt.show()

#Correlation check

df[num_cols].corr()

#Heatmap to understand the correlation

sns.heatmap(df[num_cols].corr(),annot=True,cmap='coolwarm',fmt=".2f")

#Feature selection (engineering)- Correlation matrix is taken into account.
#Given that this mushroom edibility prediction is focused mainly of physical characteristics, season does not come into the fray
#Also, the general idea is that mushrooms grown in any season can or cannot be poisonous (It has more to do with physical features to understand if it is poisonous)
#Season does not show any correlation with class here in the heatmap

df = df.drop(columns='season')

df

#SHOWS THAT THERE IS NO CATEGORICAL COLUMNS LEFT AFTER TAKING SEASON OUT OF ACCOUNT
categ_cols = df.select_dtypes(include='object').columns
categ_cols

#Encoding the categorical variables because most machine learning algorithms cannot handle categorical data directly and require numerical input
#No need for encoding here now
#In normal case, if there are categorical variables - encoding = LabelEncoder() df['categ_cols'] = encoding.fit_transform(df['categ_cols'])

#FEATURE

X = df.drop(columns='class')
X

#Target (Label)

Y = df['class']
Y

#splitting the data into train and test

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=42)

#Scaling for better optimization
#Standard Scaler - Scales to Mean 0 and Standard deviation 1

from sklearn.preprocessing import StandardScaler

scaling = StandardScaler()
X_train = scaling.fit_transform(X_train)
X_test = scaling.transform(X_test)

df['class'].value_counts() #No imbalance

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

model.fit(X_train,Y_train)

y_pred = model.predict(X_test)
y_pred

#Classification evaluation

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix


accuracy = accuracy_score(Y_test, y_pred)
print("Accuracy:", round(accuracy*100,2),"%")
prec = precision_score(Y_test, y_pred)
print("Precision:", round(prec*100,2),"%")
recall = recall_score(Y_test, y_pred)
print("Recall:", round(recall*100,2),"%")
classification_report_str = classification_report(Y_test, y_pred)
print(f"Classification Report:\n{classification_report_str}")

conf_matrix = confusion_matrix(Y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

#Good accuracy indicates the model correctly classifies a high proportion of instances overall.
#Good precision means the model makes few false positive errors, reliably identifying true positives.
#Good recall signifies the model effectively captures most actual positive instances, minimizing false negatives.

# True positives indicate that the model is accurately identifying the edibility of mushrooms, contributing to the reliability of its predictions.

from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier()
model2.fit(X_train,Y_train)

y_pred2 = model.predict(X_test)
y_pred

accuracy = accuracy_score(Y_test, y_pred2)
print("Accuracy:", round(accuracy*100,2),"%")
prec = precision_score(Y_test, y_pred2)
print("Precision:", round(prec*100,2),"%")
recall = recall_score(Y_test, y_pred2)
print("Recall:", round(recall*100,2),"%")
classification_report_str = classification_report(Y_test, y_pred2)
print(f"Classification Report:\n{classification_report_str}")

conf_matrix = confusion_matrix(Y_test, y_pred2)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

import xgboost as xgb
model3 = xgb.XGBClassifier()

model3.fit(X_train,Y_train)

y_pred3 = model3.predict(X_test)
y_pred3

accuracy = accuracy_score(Y_test, y_pred2)
print("Accuracy:", round(accuracy*100,2),"%")
prec = precision_score(Y_test, y_pred2)
print("Precision:", round(prec*100,2),"%")
recall = recall_score(Y_test, y_pred2)
print("Recall:", round(recall*100,2),"%")
classification_report_str = classification_report(Y_test, y_pred2)
print(f"Classification Report:\n{classification_report_str}")

conf_matrix = confusion_matrix(Y_test, y_pred2)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# The takeaway is that all physical features seem to contribute to edibility check because the class is dependent on each
#Season is not a physical feature of the mushroom and if physical features deem it to be not edibile, it is not edibile irrespective of the season growth

#Classification is done well here and the metrics reflect it